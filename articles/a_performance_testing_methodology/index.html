<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="Daniel Valencia Backhoff"><meta name=description content="The primary goal of any web site performance testing and capacity planning activity is to know how many users can our system handle under acceptable response times given certain hardware specifications. We can get this information by graphing a concurrency vs response time graph.
A typical graph looks like this:
Note how the curve grows exponentially. Depending on the nature of the website (or web pages) under test you might have different acceptable response times."><meta name=keywords content><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://www.danvalencia.dev/articles/a_performance_testing_methodology/><title>A Performance Testing Methodology :: A Technology Journal â€” Notes about technology and life.</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=/main.min.7bfbbe12786fa0ded4b4c0d792cbb36a5bd0bdb0b856dde57aa7b1f6fe0f2b87.css><meta itemprop=name content="A Performance Testing Methodology"><meta itemprop=description content="The primary goal of any web site performance testing and capacity planning activity is to know how many users can our system handle under acceptable response times given certain hardware specifications. We can get this information by graphing a concurrency vs response time graph.
A typical graph looks like this:
Note how the curve grows exponentially. Depending on the nature of the website (or web pages) under test you might have different acceptable response times."><meta itemprop=datePublished content="2012-10-29T10:50:03-07:00"><meta itemprop=dateModified content="2012-10-29T10:50:03-07:00"><meta itemprop=wordCount content="1133"><meta itemprop=keywords content="performance testing,"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.danvalencia.dev/"><meta name=twitter:title content="A Performance Testing Methodology"><meta name=twitter:description content="The primary goal of any web site performance testing and capacity planning activity is to know how many users can our system handle under acceptable response times given certain hardware specifications. We can get this information by graphing a concurrency vs response time graph.
A typical graph looks like this:
Note how the curve grows exponentially. Depending on the nature of the website (or web pages) under test you might have different acceptable response times."><meta property="article:published_time" content="2012-10-29 10:50:03 -0700 -0700"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo><span class=logo__mark>></span>
<span class=logo__text>cd ~</span>
<span class=logo__cursor></span></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=https://www.danvalencia.dev/about/me>About</a></li><li><a href=https://www.danvalencia.dev/articles>Articles</a></li><li><a href=https://www.danvalencia.dev/talks>Talks</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://www.danvalencia.dev/articles/a_performance_testing_methodology/>A Performance Testing Methodology</a></h2><div class=post-content><p>The primary goal of any web site performance testing and capacity planning activity is to know how many users can our system handle under acceptable response times given certain hardware specifications. We can get this information by graphing a <em>concurrency</em> vs <em>response time</em> graph.</p><p>A typical graph looks like this:</p><p><img src=http://s3.amazonaws.com/danvalencia_my_site/response_time_concurrency.png alt="Response time vs Concurrency"></p><p>Note how the curve grows exponentially. Depending on the nature of the website (or web pages) under test you might have different acceptable response times. For example, if you&rsquo;re testing a service for uploading large images asynchronously, you might be able to tolerate larger response times. In general, however, response time should be a non-functional requirement defined by your project&rsquo;s stakeholders.</p><p>I will now go through a standard methodology for obtaining a graph like this.</p><h3 id=performance-testing-concepts>Performance Testing Concepts</h3><p>Let me start by defining some important concepts:</p><ul><li><strong>Arrival Rate</strong>: The rate at which requests arrive to the system.</li><li><strong>NActive</strong>: The number of active users in the system, either being serviced or waiting.</li><li><strong>NConc</strong>: The number of concurrent users being serviced in the system. This is the number we want to graph against the response times.</li><li><strong>Service Time</strong>: The average response time for a given request when the system is under no load.</li><li><strong>Think Time</strong>: The time a user spends between requests, for example reading a webpage.</li><li><strong>Duration of Stay</strong>: The average time a user spends in the system. Also called <em>wait time</em> or <em>session duration</em>.</li></ul><h3 id=getting-useful-data>Getting Useful Data</h3><p>The first thing to do is to obtain some data:</p><ul><li>Number of visitors per month (or <em>Arrival Rate</em>) we expect.</li><li>Average time a user spends in the site (or <em>Duration of Stay</em>)</li><li>Number of pages that the user visits on average.</li></ul><p>For existing websites, these numbers are typically known or can easily be calculated by parsing web server access log files (e.g. <em>Apache access logs</em>). For new websites it&rsquo;s a little bit more trickier as you&rsquo;ll have to do some investigation. You might consider talking with your customer and business analyst.</p><p>One thing to note is that, specially for new sites, the number of expected visitors is a ballpark number and it does not need to be exact. We need it as a starting point for our concurrency calculations. In these cases, setting some assumptions is a valid thing to do.</p><p>For this example we&rsquo;ll use the following numbers:</p><ul><li>Number of visitors per month: <em>100,000</em></li><li><em>Duration of stay</em> per visitor (on average): <em>5 minutes</em></li><li>Average number of clicks per user: <em>10</em></li></ul><p>We will also assume that the bulk of the visits happens in the course of 12 hours. Often you have to do this sort of adjustments. Again, for existing websites this can be obtained by analyzing access logs, but for new websites you can make assumptions. For example, if you&rsquo;re working on an e-commerce site that sells goods in the US, the bulk of your sales will likely happen during the day.</p><h3 id=active-users>Active Users</h3><p>The first thing we need to do is to calculate the number of active users (or <em>NActive</em>) at any given time. We can obtain this using <a href=http://en.wikipedia.org/wiki/Little%27s_law target=_blank><em>Little&rsquo;s Law</em></a>:</p><blockquote><p>NActive = &lambda;W</p></blockquote><p>where <em>&lambda;</em> is the <em>arrival rate</em> and <em>W</em> is the <em>duration of stay</em> (also called <em>wait time</em>). We can calculate our arrival rate in visitors per minute by doing some simple math:</p><blockquote><p>&lambda; = 100,000 users/month
100,000/30 = 3,333 users/day<br>3,<sup>333</sup>&frasl;<sub>12</sub> = 278 users/hour
<sup>278</sup>&frasl;<sub>60</sub> = 4.63 users/minute</p></blockquote><p>With our <em>arrival rate</em> in users per minute we can obtain the number of active users at any given time by multiplying by the <em>duration of stay</em>:</p><blockquote><p>NActive = &lambda;W<br>NActive = (4.63 users/min) * (5 min)<br>NActive = 23 users</p></blockquote><p>This means that on average, there are 23 users in the system, either waiting a response from the server or doing something else. In other words, at any given time there are 23 <em>sessions</em> on the server.</p><h3 id=think-time>Think Time</h3><p>If we know the average number of <em>requests per user</em> (<em>rpu</em>) and we know the average time each user spends in the site (W), calculating the average <em>think time</em> (<em>Z</em>) or time between requests is pretty straightforward:</p><blockquote><p>Z = W/rpu<br>Z = 300sec/10<br>Z = 30 sec</p></blockquote><p>We will use this number for our <em>concurrency</em> calculation.</p><h3 id=nconc-or-concurrent-users><em>NConc</em> or concurrent users</h3><p>The number of concurrent users at any given time is a function of the number of active users and the percentage of time users are waiting for a request to complete. Mathematically speaking:</p><blockquote><p>NConc = NActive( R / ( R+Z ) )</p></blockquote><p>where <em>NConc</em> is the number of _concurrent users_a at any given time, <em>NActive</em> is the total number of users in the system, <em>R</em> is the <em>service time</em> and Z is the <em>think time</em>.</p><p>We have calculated all these numbers except the <em>service time</em>. As mentioned before, this should be known for existing websites or should be relatively easy to obtain via the logs. Another common practice is to perform benchmarking tests to obtain the average response time of each request when the system is under no load. For this example we&rsquo;ll assume that our service time is of <em>1 second</em>.</p><p>With the numbers we have, we can calculate the number of average concurrent users in our system:</p><blockquote><p>NConc = NActive( R / ( R+Z ) )<br>NConc = 23( 1 / (1 + 30) )<br>NConc = 0.74 users</p></blockquote><p>The calculated average concurrency is of <em>0.74</em> users. This makes sense given our numbers. The cool thing about this formula is that we can simulate higher concurrency just by varying the <em>think time</em>. As the think time is shortened, the concurrency increases. This is very useful when running our load tests because we are able to simulate higher loads in the system by using the same number of threads, hence saving computing resources.</p><h3 id=writing-our-load-tests>Writing our load tests</h3><p>With the information we have we&rsquo;re now ready to write our test scripts. The way we write our tests will vary depending on the tool we use (e.g. <a href=http://jmeter.apache.org/ target=_blank><em>JMeter</em></a> and <a href=http://www.hpl.hp.com/research/linux/httperf/ target=_blank><em>httperf</em></a> are both great open source tools). I&rsquo;ll write about the creation of load tests in another article, but in a nutshell, you&rsquo;ll need the following information:</p><ul><li><em>NActive</em>, or number of users (or threads) to simulate.</li><li><em>Think time</em>, or time to wait between requests.</li></ul><h3 id=getting-your-graph>Getting your graph</h3><p>What you&rsquo;ll want to do is then run the load tests for some time to obtain average response times as well as <em>std. dev</em>, <em>median</em>, <em>90th</em> and <em>99th</em> percentiles. Five minutes is a good rule of thumb for running your load tests, but you might also consider running longer tests for stressing the system. Then, you&rsquo;ll want to run your tests again using higher loads (e.g. by varying the <em>think time</em> or number of threads). You&rsquo;ll want repeat this several times (e.g. 5 - 10 times) until you have enough data points to plot a nice graph.</p></div></article><hr><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83.0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=tag><a href=https://www.danvalencia.dev/tags/performance-testing>performance testing</a></span></p></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2020</span>
<span><a href=https://www.danvalencia.dev/>Daniel Valencia Backhoff</a></span>
<span></span><span><a href=https://www.danvalencia.dev/posts/index.xml target=_blank title=rss><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"/><path d="M4 4a16 16 0 0 1 16 16"/><circle cx="5" cy="19" r="1"/></svg></a></span></div></div><div class=footer__inner><div class=footer__content><span>Made with &#10084; by <a href=https://github.com/danvalencia>danvalencia</a></span></div></div></footer></div><script type=text/javascript src=/bundle.min.2d5469329143160ae2456a69c3c76dc2d0a3b212b46afe291a51bd68650ed6f8697e001dab54f1c272c77ce08092a8c55e5bb4314e0ee334aab4b927ec896638.js integrity="sha512-LVRpMpFDFgriRWppw8dtwtCjshK0av4pGlG9aGUO1vhpfgAdq1TxwnLHfOCAkqjFXlu0MU4O4zSqtLkn7IlmOA=="></script></body></html>